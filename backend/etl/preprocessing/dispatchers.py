from loguru import logger
from pydantic.type_adapter import P

from .cleaning_data_handlers import (
    CleaningDataHandler,
    ArticleCleaningHandler,
    YoutubeCleaningHandler,
    RepositoryCleaningHandler,
    PDFCleaningHandler,
)

from .chunking_data_handlers import (
    ChunkingDataHandler,
    ArticleChunkingHandler,
    PDFChunkingHandler,
    YoutubeChunkingHandler,
    RepositoryChunkingHandler,
)
from .embedding_data_handlers import (
    EmbeddingDataHandler,
    ArticleEmbeddingHandler,
    RepositoryEmbeddingHandler,
    YoutubeEmbeddingHandler,
    PDFEmbeddingHandler,
)

from backend.etl.domain.documents import (
    NoSQLBaseDocument,
    RepositoryDocument,
    YoutubeDocument,
)
from backend.etl.domain.cleaned_documents import VectorBaseDocument
from backend.etl.domain.types import DataCategory


class CleaningHandlerFactory:
    @staticmethod
    def create_handler(data_category: DataCategory) -> CleaningDataHandler:
        if data_category == DataCategory.ARTICLES:
            return ArticleCleaningHandler()
        elif data_category == DataCategory.YOUTUBEVIDEOS:
            return YoutubeCleaningHandler()
        elif data_category == DataCategory.REPOSITORIES:
            return RepositoryCleaningHandler()
        elif data_category == DataCategory.PDFS:
            return PDFCleaningHandler()
        else:
            raise ValueError("Unsupported data type")


class CleaningDispatcher:
    factory = CleaningHandlerFactory()

    @classmethod
    def dispatch(cls, data_model: NoSQLBaseDocument) -> VectorBaseDocument:
        data_category = DataCategory(data_model.get_collection_name())
        handler = cls.factory.create_handler(data_category)
        clean_model = handler.clean(data_model)

        logger.info(
            "Document cleaned successfully.",
            data_category=data_category,
            cleaned_content_len=len(clean_model.content),
        )

        return clean_model


class ChunkingHandlerFactory:
    @staticmethod
    def create_handler(data_category: DataCategory) -> ChunkingDataHandler:
        if data_category == DataCategory.ARTICLES:
            return ArticleChunkingHandler()
        elif data_category == DataCategory.YOUTUBEVIDEOS:
            return YoutubeChunkingHandler()
        elif data_category == DataCategory.REPOSITORIES:
            return RepositoryChunkingHandler()
        elif data_category == DataCategory.PDFS:
            return PDFChunkingHandler()
        else:
            raise ValueError("Unsupported data type")


class ChunkingDispatcher:
    factory = ChunkingHandlerFactory

    @classmethod
    def dispatch(cls, data_model: VectorBaseDocument) -> list[VectorBaseDocument]:
        data_category = data_model.get_category()
        handler = cls.factory.create_handler(data_category)
        chunk_models = handler.chunk(data_model)

        logger.info(
            "Document chunked successfully.",
            num=len(chunk_models),
            data_category=data_category,
        )

        return chunk_models


class EmbeddingHandlerFactory:
    @staticmethod
    def create_handler(data_category: DataCategory) -> EmbeddingDataHandler:

        if data_category == DataCategory.ARTICLES:
            return ArticleEmbeddingHandler()
        elif data_category == DataCategory.YOUTUBEVIDEOS:
            return YoutubeEmbeddingHandler()
        elif data_category == DataCategory.REPOSITORIES:
            return RepositoryEmbeddingHandler()
        elif data_category == DataCategory.PDFS:
            return PDFEmbeddingHandler()
        else:
            raise ValueError("Unsupported data type")


class EmbeddingDispatcher:
    factory = EmbeddingHandlerFactory

    @classmethod
    def dispatch(
        cls, data_model: VectorBaseDocument | list[VectorBaseDocument]
    ) -> VectorBaseDocument | list[VectorBaseDocument]:
        is_list = isinstance(data_model, list)
        if not is_list:
            data_model = [data_model]

        if len(data_model) == 0:
            return []

        data_category = data_model[0].get_category()
        assert all(
            data_model.get_category() == data_category for data_model in data_model
        ), "Data models must be of the same category."
        handler = cls.factory.create_handler(data_category)

        embedded_chunk_model = handler.embed_batch(data_model)

        if not is_list:
            embedded_chunk_model = embedded_chunk_model[0]

        logger.info(
            "Data embedded successfully.",
            data_category=data_category,
        )

        return embedded_chunk_model


if __name__ == "__main__":
    # Example usage
    from backend.etl.domain.documents import (
        RepositoryDocument,
        PDFDocument,
        ArticleDocument,
    )

    example_data = YoutubeDocument().find()

    cleaned_doc = CleaningDispatcher.dispatch(example_data)

    # with open("cleaned_doc.txt", "w") as f:
    #     f.write(cleaned_doc.content)

    chunked_docs = ChunkingDispatcher.dispatch(cleaned_doc)
    print(len(chunked_docs))
    embedded_docs = EmbeddingDispatcher.dispatch(chunked_docs)
    print(len(embedded_docs))
    print(len(embedded_docs[0].content))
